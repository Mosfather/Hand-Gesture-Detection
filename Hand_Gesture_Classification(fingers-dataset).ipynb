{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plaidml.keras\n",
    "import os\n",
    "plaidml.keras.install_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'plaidml.keras.backend' from 'C:\\\\Users\\\\Bamouh Mohamed\\\\AppData\\\\Local\\\\conda\\\\conda\\\\envs\\\\Projectenv\\\\lib\\\\site-packages\\\\plaidml\\\\keras\\\\backend.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "from itertools import chain\n",
    "from pynput.keyboard import Key, Controller\n",
    "from keras.models import model_from_json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3,3),np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathTest = \"fingers/test/*\"\n",
    "NUMBER_OF_CLASSES = 6\n",
    "#indice_to_label = {0:\"0L\", 1:\"1L\", 2:\"2L\", 3:\"3L\", 4:\"4L\", 5:\"5L\", 6:\"0R\", 7:\"1R\", 8:\"2R\", 9:\"3R\", 10:\"4R\", 11:\"5R\"}\n",
    "indice_to_label = {0:\"0\", 1:\"1\", 2:\"2\", 3:\"3\", 4:\"4\", 5:\"5\"}\n",
    "label_to_indice = {v:k for k,v in indice_to_label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = np.array([0, 48, 80], dtype = \"uint8\")\n",
    "upper = np.array([20, 255, 255], dtype = \"uint8\")\n",
    "\n",
    "lower_gray = np.array([42], dtype = \"uint8\")\n",
    "upper_gray = np.array([176], dtype = \"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_from_video(img):\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = cv2.GaussianBlur(img,(5,5),0)\n",
    "    _,img = cv2.threshold(img,20,255,cv2.THRESH_BINARY)\n",
    "    #kernel = np.ones((3,3),np.uint8)\n",
    "    #img = cv2.dilate(img, kernel, iterations = 1)\n",
    "    #img = cv2.erode(img, kernel, iterations = 1)\n",
    "    img = img/255\n",
    "    return img\n",
    "\n",
    "def process_image_for_training(img):\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = cv2.GaussianBlur(img,(5,5),0)\n",
    "    _,img = cv2.threshold(img,80,255,cv2.THRESH_BINARY)\n",
    "    img = img/255\n",
    "    return img\n",
    "\n",
    "def img_to_sample(img):\n",
    "    img = img.astype(np.uint8)\n",
    "    img = np.reshape(img, (128, 128))\n",
    "    \n",
    "    img = process_image_for_training(img)\n",
    "    \n",
    "    img = np.reshape(img, (128, 128, 1))\n",
    "    return img\n",
    "\n",
    "def loadModel(architecturePath=\"handmodel_fingers_architecture.json\", weightsPath=\"handmodel_fingers_weights.hdf5\"):\n",
    "    # Model reconstruction from JSON file\n",
    "    with open(architecturePath, 'r') as f:\n",
    "        model = model_from_json(f.read())\n",
    "\n",
    "    # Load weights into the new model\n",
    "    model.load_weights(weightsPath)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"opencl_amd_iceland.0\"\n"
     ]
    }
   ],
   "source": [
    "model = loadModel(architecturePath=\"handmodel_fingers_architecture.json\", weightsPath=\"handmodel_fingers_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "keyboard = Controller()\n",
    "\n",
    "lower = np.array([0, 48, 80], dtype = \"uint8\")\n",
    "upper = np.array([20, 255, 255], dtype = \"uint8\")\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.flip(frame,1)\n",
    "    \n",
    "    left_zone = gray[100:350, 10:260, :]\n",
    "    cv2.rectangle(gray,(10,100),(260,350),(0,255,0),3)\n",
    "    \n",
    "    right_zone = gray[100:350, 400:650, :]\n",
    "    cv2.rectangle(gray,(400,100),(650,350),(0,255,0),3)\n",
    "    \n",
    "    left_zone = cv2.cvtColor(left_zone, cv2.COLOR_BGR2HSV)\n",
    "    right_zone = cv2.cvtColor(right_zone, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    cv2.imshow('HSV left',left_zone)\n",
    "    cv2.imshow('HSV right',right_zone)\n",
    "    \n",
    "    skinMask_left = cv2.inRange(left_zone, lower, upper)\n",
    "    skinMask_right = cv2.inRange(right_zone, lower, upper)\n",
    "    \n",
    "    skinMask_left = cv2.dilate(skinMask_left, kernel, iterations = 2)\n",
    "    skinMask_left = cv2.erode(skinMask_left, kernel, iterations = 2)\n",
    "    skinMask_left = cv2.GaussianBlur(skinMask_left, (3, 3), 0)\n",
    "    \n",
    "    skinMask_right = cv2.dilate(skinMask_right, kernel, iterations = 2)\n",
    "    skinMask_right = cv2.erode(skinMask_right, kernel, iterations = 2)\n",
    "    skinMask_right = cv2.GaussianBlur(skinMask_right, (3, 3), 0)\n",
    "    \n",
    "    left_zone = cv2.bitwise_and(left_zone, left_zone, mask = skinMask_left)\n",
    "    right_zone = cv2.bitwise_and(right_zone, right_zone, mask = skinMask_right)\n",
    "       \n",
    "    #cv2.rectangle(gray,(10,100),(138,228),(0,255,0),3)\n",
    "    \n",
    "    # Capture the hand zone\n",
    "    #zone = gray[100:228, 10:138]\n",
    "    #zone = gray[100:350, 10:260, :]\n",
    "    left_zone = cv2.cvtColor(left_zone, cv2.COLOR_BGR2GRAY)\n",
    "    right_zone = cv2.cvtColor(right_zone, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    left_zone = process_image_from_video(left_zone)\n",
    "    right_zone = process_image_from_video(right_zone)\n",
    "    \n",
    "    left_zone_pred = left_zone.reshape((1, 128, 128, 1))\n",
    "    right_zone_pred = right_zone.reshape((1, 128, 128, 1))\n",
    "    \n",
    "    \n",
    "    left_zone_pred = model.predict(left_zone_pred)\n",
    "    right_zone_pred = model.predict(right_zone_pred)\n",
    "    \n",
    "    #categorical\n",
    "    predicted_class_left = np.argmax(left_zone_pred)\n",
    "    predicted_class_right = np.argmax(right_zone_pred)\n",
    "    \n",
    "    labels_left = [\"{} : {}%\".format(k, format(left_zone_pred[0][v]*100, '.2f')) for k,v in label_to_indice.items()]\n",
    "    labels_right = [\"{} : {}%\".format(k, format(right_zone_pred[0][v]*100, '.2f')) for k,v in label_to_indice.items()]\n",
    "    \n",
    "    if left_zone[64,64] == 255:\n",
    "        #keyboard.press(Key.left) \n",
    "        None\n",
    "    elif right_zone[64,64] == 255:\n",
    "        #keyboard.press(Key.right)\n",
    "        None\n",
    "    else:\n",
    "        #keyboard.release(Key.right)\n",
    "        #keyboard.release(Key.left)\n",
    "        None\n",
    "        \n",
    "    if predicted_class_left == 0 or predicted_class_right == 0:\n",
    "        #keyboard.press(Key.space)\n",
    "        None\n",
    "    else:\n",
    "        #keyboard.release(Key.space)\n",
    "        None\n",
    "    \n",
    "    cv2.imshow('What the Network Sees left',left_zone)\n",
    "    cv2.imshow('What the Network Sees right',right_zone)\n",
    "    \n",
    "    y = 100\n",
    "    cv2.putText(gray,indice_to_label[int(predicted_class_left)],(270,y), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,0,0),2,cv2.LINE_AA)\n",
    "    for label in labels_left:\n",
    "        y = y + 20\n",
    "        cv2.putText(gray, label,(270, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,0,0),2,cv2.LINE_AA)\n",
    "        \n",
    "    yy = 250\n",
    "    cv2.putText(gray,indice_to_label[int(predicted_class_right)],(270,yy), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,0,0),2,cv2.LINE_AA)\n",
    "    for label in labels_right:\n",
    "        yy = yy + 20\n",
    "        cv2.putText(gray, label,(270, yy), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,0,0),2,cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame      \n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF ==ord('q'):\n",
    "        break                                                                                                                     \n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
